{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:23.969960Z","iopub.status.busy":"2023-09-20T10:53:23.969597Z","iopub.status.idle":"2023-09-20T10:53:23.977224Z","shell.execute_reply":"2023-09-20T10:53:23.976179Z","shell.execute_reply.started":"2023-09-20T10:53:23.969928Z"}},"outputs":[],"source":["#function for knn model using and check accuarcy\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","def knn(train_x,train_y,valid_x,valid_y):\n","    model = RandomForestClassifier(n_estimators=100)\n","    model.fit(train_x, train_y)\n","    y_pred = model.predict(valid_x)\n","    accuracy = accuracy_score(valid_y, y_pred)\n","    print(f'Accuracy using knn: {accuracy:.2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:23.981712Z","iopub.status.busy":"2023-09-20T10:53:23.980679Z","iopub.status.idle":"2023-09-20T10:53:23.992404Z","shell.execute_reply":"2023-09-20T10:53:23.991486Z","shell.execute_reply.started":"2023-09-20T10:53:23.981682Z"}},"outputs":[],"source":["#function for data preprocessing\n","from sklearn.preprocessing import StandardScaler\n","def preprocessing(label):\n","    train=pd.read_csv(\"train.csv\")\n","    test=pd.read_csv(\"test.csv\")\n","    valid=pd.read_csv(\"valid.csv\")\n","    train_x_label_1=train.iloc[:, :-4]\n","    train_y_label_1=train.iloc[:,-5+label]\n","\n","    ss = StandardScaler()\n","    scaled_train_x_label = ss.fit_transform(train_x_label_1)\n","    scaled_train_x_label\n","    scaled_test_x_label=ss.fit_transform(test_x_label_1)\n","    scaled_valid_x_label=ss.fit_transform(valid_x_label_1)\n","    return scaled_train_x_label,scaled_valid_x_label\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# scaling function\n","from sklearn.preprocessing import StandardScaler\n","def scale(train_x,valid_x):\n","    ss = StandardScaler()\n","    scaled_train_x_label = ss.fit_transform(train_x)\n","    scaled_train_x_label\n","    scaled_test_x_label=ss.fit_transform(test_x)\n","    scaled_valid_x_label=ss.fit_transform(valid_x)\n","    return scaled_train_x_label,scaled_valid_x_label\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:23.993749Z","iopub.status.busy":"2023-09-20T10:53:23.993492Z","iopub.status.idle":"2023-09-20T10:53:24.004209Z","shell.execute_reply":"2023-09-20T10:53:24.003287Z","shell.execute_reply.started":"2023-09-20T10:53:23.993727Z"}},"outputs":[],"source":["# pca approch\n","from sklearn.decomposition import PCA\n","def pca(train_x,train_y,valid_x,valid_y,test_x):\n","    pca=PCA(.95, svd_solver='full')\n","    pca=pca.fit(train_x)\n","    train_features_pca=pca.transform(train_x)\n","    valid_features_pca=pca.transform(valid_x)\n","    test_features_pca=pca.transform(test_x)\n","    print(\"accuarcy after pca\")\n","    #     knn(train_features_pca,train_y,valid_features_pca,valid_y)\n","    return train_features_pca,valid_features_pca,test_x_features\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:24.006015Z","iopub.status.busy":"2023-09-20T10:53:24.005690Z","iopub.status.idle":"2023-09-20T10:53:24.014220Z","shell.execute_reply":"2023-09-20T10:53:24.013252Z","shell.execute_reply.started":"2023-09-20T10:53:24.005985Z"}},"outputs":[],"source":["# Write predicted values to a CSV file.\n","import pandas as pd\n","\n","def write_predictions_to_csv(predictions, output_file):\n","\n","    \n","    # Create a DataFrame with a column for predictions\n","    df = pd.DataFrame({'Predicted_Label': predictions})\n","    \n","    # Save the DataFrame to a CSV file\n","    df.to_csv(output_file, index=False)  # Set index=False to exclude row numbers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:24.016260Z","iopub.status.busy":"2023-09-20T10:53:24.015908Z","iopub.status.idle":"2023-09-20T10:53:36.138433Z","shell.execute_reply":"2023-09-20T10:53:36.137368Z","shell.execute_reply.started":"2023-09-20T10:53:24.016212Z"}},"outputs":[],"source":["train=pd.read_csv(\"/kaggle/input/ml-grp-project-train-data-set/train.csv\")\n","test=pd.read_csv(\"/kaggle/input/ml-grp-project-test-data-set/test.csv\")\n","valid=pd.read_csv(\"/kaggle/input/ml-grp-project-test-data-set/valid.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:36.140337Z","iopub.status.busy":"2023-09-20T10:53:36.139973Z","iopub.status.idle":"2023-09-20T10:53:36.181439Z","shell.execute_reply":"2023-09-20T10:53:36.180415Z","shell.execute_reply.started":"2023-09-20T10:53:36.140305Z"}},"outputs":[],"source":["train.head()\n","test.head()\n","valid.head()\n","train.info()\n","train.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:36.315021Z","iopub.status.busy":"2023-09-20T10:53:36.314044Z","iopub.status.idle":"2023-09-20T10:53:37.736734Z","shell.execute_reply":"2023-09-20T10:53:37.735634Z","shell.execute_reply.started":"2023-09-20T10:53:36.314968Z"}},"outputs":[],"source":["#Check is there any NaN values\n","\n","train.isnull().sum()\n","\n","#check is there any duplicates in the data set\n","train.drop_duplicates()\n","\n","\n","#only label_2 has NaN values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-20T10:53:37.738752Z","iopub.status.busy":"2023-09-20T10:53:37.738310Z","iopub.status.idle":"2023-09-20T10:53:46.977968Z","shell.execute_reply":"2023-09-20T10:53:46.976128Z","shell.execute_reply.started":"2023-09-20T10:53:37.738716Z"}},"outputs":[],"source":["#cheack is there any string values, if there any string values we can encode the values.\n","contains_strings=train.applymap(lambda x: isinstance(x, str))\n","if contains_strings.any().any():\n","    print(\"There are string values in the DataFrame columns.\")\n","else:\n","    print(\"There are no string values in the DataFrame columns.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# hyper parameter tuninng\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","def logistic_regression_hyper_parameter(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1):\n","    param_grid = {\n","        'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Adjust the range based on your needs\n","        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n","        'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],\n","        'max_iter': [100, 200, 300]  # Adjust the range based on your needs\n","    }\n","\n","    # Create a Logistic Regression classifier\n","    logistic_regression = LogisticRegression()\n","\n","    # Perform Grid Search with cross-validation (e.g., 5-fold cross-validation)\n","    grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, scoring='accuracy', cv=5)\n","\n","    # Fit the Grid Search to your training data\n","    grid_search.fit(scaled_train_x_label_1_df_pca,train_y_label_1)\n","\n","    # Get the best hyperparameters\n","    best_params = grid_search.best_params_\n","    print(\"Best Hyperparameters:\", best_params)\n","\n","    # Get the best model\n","    best_model = grid_search.best_estimator_\n","\n","    # Evaluate the best model on the validation data\n","    accuracy = best_model.score(scaled_valid_x_label_1_df_pca, valid_y_label_1)\n","    print(\"Validation Accuracy with Best Model:\", accuracy)\n","    y_pred= best_model.predict(scaled_valid_x_label_1_df_pca)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**label 2**"]},{"cell_type":"markdown","metadata":{},"source":["impute the nan al"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T06:41:44.375952Z","iopub.status.busy":"2023-09-24T06:41:44.375167Z","iopub.status.idle":"2023-09-24T06:41:57.024335Z","shell.execute_reply":"2023-09-24T06:41:57.023434Z","shell.execute_reply.started":"2023-09-24T06:41:44.375915Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_15/350447373.py:27: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data_missing_target['label_2'] = predicted_target_values.round(decimals=0)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>feature_6</th>\n","      <th>feature_7</th>\n","      <th>feature_8</th>\n","      <th>feature_9</th>\n","      <th>feature_10</th>\n","      <th>...</th>\n","      <th>feature_763</th>\n","      <th>feature_764</th>\n","      <th>feature_765</th>\n","      <th>feature_766</th>\n","      <th>feature_767</th>\n","      <th>feature_768</th>\n","      <th>label_1</th>\n","      <th>label_2</th>\n","      <th>label_3</th>\n","      <th>label_4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>480</th>\n","      <td>0.011657</td>\n","      <td>0.038439</td>\n","      <td>0.103671</td>\n","      <td>-0.016977</td>\n","      <td>-0.000953</td>\n","      <td>-0.003522</td>\n","      <td>-0.065092</td>\n","      <td>-0.028373</td>\n","      <td>-0.079483</td>\n","      <td>0.038152</td>\n","      <td>...</td>\n","      <td>0.010156</td>\n","      <td>0.002009</td>\n","      <td>0.019304</td>\n","      <td>0.015793</td>\n","      <td>-0.012956</td>\n","      <td>0.038805</td>\n","      <td>5</td>\n","      <td>25.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>481</th>\n","      <td>0.164148</td>\n","      <td>0.091572</td>\n","      <td>0.228022</td>\n","      <td>-0.096475</td>\n","      <td>-0.041760</td>\n","      <td>0.011285</td>\n","      <td>-0.053629</td>\n","      <td>-0.052081</td>\n","      <td>-0.203983</td>\n","      <td>0.119327</td>\n","      <td>...</td>\n","      <td>-0.105542</td>\n","      <td>-0.097742</td>\n","      <td>0.003595</td>\n","      <td>0.043899</td>\n","      <td>0.016852</td>\n","      <td>0.190699</td>\n","      <td>5</td>\n","      <td>25.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>482</th>\n","      <td>0.038285</td>\n","      <td>0.055738</td>\n","      <td>0.124604</td>\n","      <td>-0.041057</td>\n","      <td>-0.022140</td>\n","      <td>0.014445</td>\n","      <td>-0.050445</td>\n","      <td>-0.038413</td>\n","      <td>-0.069275</td>\n","      <td>0.048514</td>\n","      <td>...</td>\n","      <td>-0.008590</td>\n","      <td>-0.020720</td>\n","      <td>-0.016982</td>\n","      <td>0.014780</td>\n","      <td>-0.020130</td>\n","      <td>0.045188</td>\n","      <td>5</td>\n","      <td>25.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>483</th>\n","      <td>0.179149</td>\n","      <td>0.076565</td>\n","      <td>0.324054</td>\n","      <td>-0.159201</td>\n","      <td>-0.071644</td>\n","      <td>-0.071874</td>\n","      <td>-0.043605</td>\n","      <td>-0.079920</td>\n","      <td>-0.287324</td>\n","      <td>0.054799</td>\n","      <td>...</td>\n","      <td>-0.124709</td>\n","      <td>-0.052751</td>\n","      <td>-0.006879</td>\n","      <td>0.160734</td>\n","      <td>0.011273</td>\n","      <td>0.142608</td>\n","      <td>5</td>\n","      <td>25.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>484</th>\n","      <td>0.041122</td>\n","      <td>0.054127</td>\n","      <td>0.110883</td>\n","      <td>-0.023344</td>\n","      <td>-0.008510</td>\n","      <td>0.003221</td>\n","      <td>-0.023545</td>\n","      <td>-0.001871</td>\n","      <td>-0.077041</td>\n","      <td>0.030782</td>\n","      <td>...</td>\n","      <td>-0.004103</td>\n","      <td>0.000851</td>\n","      <td>-0.019949</td>\n","      <td>0.005263</td>\n","      <td>-0.002535</td>\n","      <td>0.052611</td>\n","      <td>5</td>\n","      <td>25.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>475</th>\n","      <td>-0.002498</td>\n","      <td>0.056692</td>\n","      <td>0.099553</td>\n","      <td>-0.049667</td>\n","      <td>0.009203</td>\n","      <td>0.003939</td>\n","      <td>-0.033011</td>\n","      <td>-0.045058</td>\n","      <td>-0.088114</td>\n","      <td>0.056637</td>\n","      <td>...</td>\n","      <td>-0.047962</td>\n","      <td>-0.063162</td>\n","      <td>0.007086</td>\n","      <td>0.039538</td>\n","      <td>-0.035178</td>\n","      <td>0.075493</td>\n","      <td>45</td>\n","      <td>27.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>476</th>\n","      <td>0.000798</td>\n","      <td>0.042407</td>\n","      <td>0.134658</td>\n","      <td>-0.063675</td>\n","      <td>0.006260</td>\n","      <td>0.005855</td>\n","      <td>-0.041181</td>\n","      <td>-0.002418</td>\n","      <td>-0.065730</td>\n","      <td>0.054698</td>\n","      <td>...</td>\n","      <td>-0.010955</td>\n","      <td>-0.019270</td>\n","      <td>0.011839</td>\n","      <td>-0.011412</td>\n","      <td>-0.042060</td>\n","      <td>0.066006</td>\n","      <td>45</td>\n","      <td>26.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>477</th>\n","      <td>0.123867</td>\n","      <td>0.202822</td>\n","      <td>0.252987</td>\n","      <td>-0.185939</td>\n","      <td>-0.010909</td>\n","      <td>0.000286</td>\n","      <td>0.016383</td>\n","      <td>-0.079745</td>\n","      <td>-0.224522</td>\n","      <td>0.144664</td>\n","      <td>...</td>\n","      <td>-0.096495</td>\n","      <td>-0.095562</td>\n","      <td>-0.041426</td>\n","      <td>0.128829</td>\n","      <td>-0.102785</td>\n","      <td>0.109528</td>\n","      <td>45</td>\n","      <td>30.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>0.000865</td>\n","      <td>0.037075</td>\n","      <td>0.067764</td>\n","      <td>-0.070174</td>\n","      <td>0.001491</td>\n","      <td>0.000797</td>\n","      <td>-0.023979</td>\n","      <td>0.019960</td>\n","      <td>-0.073889</td>\n","      <td>0.040104</td>\n","      <td>...</td>\n","      <td>0.012223</td>\n","      <td>-0.041105</td>\n","      <td>-0.008145</td>\n","      <td>0.007105</td>\n","      <td>0.001356</td>\n","      <td>0.043914</td>\n","      <td>45</td>\n","      <td>31.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>0.042526</td>\n","      <td>0.072965</td>\n","      <td>0.158634</td>\n","      <td>-0.049048</td>\n","      <td>-0.021173</td>\n","      <td>0.004486</td>\n","      <td>-0.085393</td>\n","      <td>-0.000150</td>\n","      <td>-0.194105</td>\n","      <td>0.077327</td>\n","      <td>...</td>\n","      <td>-0.084374</td>\n","      <td>-0.052034</td>\n","      <td>0.005006</td>\n","      <td>0.020111</td>\n","      <td>-0.054809</td>\n","      <td>0.096537</td>\n","      <td>45</td>\n","      <td>31.0</td>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28520 rows × 772 columns</p>\n","</div>"],"text/plain":["     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n","480   0.011657   0.038439   0.103671  -0.016977  -0.000953  -0.003522   \n","481   0.164148   0.091572   0.228022  -0.096475  -0.041760   0.011285   \n","482   0.038285   0.055738   0.124604  -0.041057  -0.022140   0.014445   \n","483   0.179149   0.076565   0.324054  -0.159201  -0.071644  -0.071874   \n","484   0.041122   0.054127   0.110883  -0.023344  -0.008510   0.003221   \n","..         ...        ...        ...        ...        ...        ...   \n","475  -0.002498   0.056692   0.099553  -0.049667   0.009203   0.003939   \n","476   0.000798   0.042407   0.134658  -0.063675   0.006260   0.005855   \n","477   0.123867   0.202822   0.252987  -0.185939  -0.010909   0.000286   \n","478   0.000865   0.037075   0.067764  -0.070174   0.001491   0.000797   \n","479   0.042526   0.072965   0.158634  -0.049048  -0.021173   0.004486   \n","\n","     feature_7  feature_8  feature_9  feature_10  ...  feature_763  \\\n","480  -0.065092  -0.028373  -0.079483    0.038152  ...     0.010156   \n","481  -0.053629  -0.052081  -0.203983    0.119327  ...    -0.105542   \n","482  -0.050445  -0.038413  -0.069275    0.048514  ...    -0.008590   \n","483  -0.043605  -0.079920  -0.287324    0.054799  ...    -0.124709   \n","484  -0.023545  -0.001871  -0.077041    0.030782  ...    -0.004103   \n","..         ...        ...        ...         ...  ...          ...   \n","475  -0.033011  -0.045058  -0.088114    0.056637  ...    -0.047962   \n","476  -0.041181  -0.002418  -0.065730    0.054698  ...    -0.010955   \n","477   0.016383  -0.079745  -0.224522    0.144664  ...    -0.096495   \n","478  -0.023979   0.019960  -0.073889    0.040104  ...     0.012223   \n","479  -0.085393  -0.000150  -0.194105    0.077327  ...    -0.084374   \n","\n","     feature_764  feature_765  feature_766  feature_767  feature_768  label_1  \\\n","480     0.002009     0.019304     0.015793    -0.012956     0.038805        5   \n","481    -0.097742     0.003595     0.043899     0.016852     0.190699        5   \n","482    -0.020720    -0.016982     0.014780    -0.020130     0.045188        5   \n","483    -0.052751    -0.006879     0.160734     0.011273     0.142608        5   \n","484     0.000851    -0.019949     0.005263    -0.002535     0.052611        5   \n","..           ...          ...          ...          ...          ...      ...   \n","475    -0.063162     0.007086     0.039538    -0.035178     0.075493       45   \n","476    -0.019270     0.011839    -0.011412    -0.042060     0.066006       45   \n","477    -0.095562    -0.041426     0.128829    -0.102785     0.109528       45   \n","478    -0.041105    -0.008145     0.007105     0.001356     0.043914       45   \n","479    -0.052034     0.005006     0.020111    -0.054809     0.096537       45   \n","\n","     label_2  label_3  label_4  \n","480     25.0        1        6  \n","481     25.0        1        6  \n","482     25.0        1        6  \n","483     25.0        1        6  \n","484     25.0        1        6  \n","..       ...      ...      ...  \n","475     27.0        1        6  \n","476     26.0        1        6  \n","477     30.0        1        6  \n","478     31.0        1        6  \n","479     31.0        1        6  \n","\n","[28520 rows x 772 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","# Load your dataset (replace 'your_dataset.csv' with your actual dataset)\n","data = pd.read_csv('/kaggle/input/ml-grp-project-layer-11/train.csv')\n","\n","# Separate the dataset into two parts: one with missing target values and one without\n","data_missing_target = data[data['label_2'].isna()]\n","data_not_missing_target = data[~data['label_2'].isna()]\n","\n","# Define features (X) and the target variable (y) for regression\n","X = data_not_missing_target.drop(columns=['label_2','label_1','label_3','label_4'])  # Features\n","y = data_not_missing_target['label_2']  # Target variable\n","\n","\n","# Train a Linear Regression model\n","regression_model = LinearRegression()\n","regression_model.fit(X, y)\n","\n","# Use the trained model to predict missing target values\n","X_missing_target = data_missing_target.drop(columns=['label_2','label_1','label_3','label_4'])  # Features for rows with missing targets\n","predicted_target_values = regression_model.predict(X_missing_target)\n","\n","\n","# Update the original dataset with imputed target values\n","data_missing_target['label_2'] = predicted_target_values.round(decimals=0)\n","\n","# Concatenate the two datasets back together\n","imputed_data = pd.concat([data_not_missing_target, data_missing_target])\n","\n","# Now 'imputed_data' contains the original data with missing target values imputed\n","imputed_data\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T06:41:57.026460Z","iopub.status.busy":"2023-09-24T06:41:57.026132Z","iopub.status.idle":"2023-09-24T06:41:59.204281Z","shell.execute_reply":"2023-09-24T06:41:59.203233Z","shell.execute_reply.started":"2023-09-24T06:41:57.026433Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["       feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n","0      -0.264047  -0.359851  -0.274026   0.821762   0.356691  -0.304230   \n","1       2.884561   0.721572   1.439809  -0.794921  -1.033501   0.160427   \n","2       0.285747  -0.007754   0.014476   0.332069  -0.365082   0.259585   \n","3       3.194304   0.416132   2.763336  -2.070524  -2.051597  -2.449116   \n","4       0.344333  -0.040537  -0.174634   0.692284   0.099244  -0.092630   \n","...          ...        ...        ...        ...        ...        ...   \n","28515  -0.556329   0.011655  -0.330786   0.156973   0.702685  -0.070105   \n","28516  -0.488278  -0.279072   0.153043  -0.127903   0.602416  -0.009989   \n","28517   2.052837   2.985809   1.783876  -2.614269   0.017521  -0.184730   \n","28518  -0.486887  -0.387611  -0.768905  -0.260055   0.439947  -0.168709   \n","28519   0.373329   0.342860   0.483485   0.169561  -0.332160  -0.052951   \n","\n","       feature_7  feature_8  feature_9  feature_10  ...  feature_759  \\\n","0      -0.783149  -0.073321   0.015944   -0.228623  ...     0.461149   \n","1      -0.402621  -0.750304  -1.595971    2.274142  ...    -0.787517   \n","2      -0.296924  -0.360001   0.148116    0.090847  ...    -0.001494   \n","3      -0.069845  -1.545236  -2.675005    0.284628  ...    -1.785065   \n","4       0.596092   0.683448   0.047561   -0.455877  ...     0.671600   \n","...          ...        ...        ...         ...  ...          ...   \n","28515   0.281842  -0.549759  -0.095796    0.341301  ...    -0.679542   \n","28516   0.010626   0.667820   0.194007    0.281508  ...     0.099126   \n","28517   1.921632  -1.540243  -1.861892    3.055327  ...    -1.546055   \n","28518   0.581700   1.306815   0.088371   -0.168457  ...     0.623177   \n","28519  -1.457112   0.732577  -1.468076    0.979200  ...    -0.551205   \n","\n","       feature_760  feature_761  feature_762  feature_763  feature_764  \\\n","0        -0.017013    -0.083796    -0.585580     0.332726     1.203918   \n","1         1.824857     0.149106    -2.108761    -1.792170    -1.842846   \n","2        -0.115111    -0.184717    -0.086436    -0.011571     0.509687   \n","3         0.340977    -0.171098    -2.764932    -2.144197    -0.468666   \n","4        -0.570444    -0.272068     0.597034     0.070853     1.168536   \n","...            ...          ...          ...          ...          ...   \n","28515    -0.058381    -0.564614    -1.287866    -0.734670    -0.786644   \n","28516    -0.919742    -0.005780    -0.384798    -0.054998     0.553976   \n","28517     1.498933     3.155988    -1.138462    -1.626019    -1.776289   \n","28518    -0.431697    -0.398180     0.578998     0.370686    -0.112938   \n","28519    -0.031498     0.057103    -0.656874    -1.403417    -0.446771   \n","\n","       feature_765  feature_766  feature_767  feature_768  \n","0         0.825874    -0.288802     0.256176    -0.498145  \n","1         0.285416     0.179635     1.022617     2.404308  \n","2        -0.422521    -0.305685     0.071706    -0.376178  \n","3        -0.074952     2.126900     0.879169     1.485357  \n","4        -0.524600    -0.464314     0.524125    -0.234337  \n","...            ...          ...          ...          ...  \n","28515     0.405519     0.106944    -0.315221     0.202906  \n","28516     0.569058    -0.742231    -0.492199     0.021618  \n","28517    -1.263497     1.595153    -2.053621     0.853254  \n","28518    -0.118477    -0.433603     0.624188    -0.400528  \n","28519     0.333974    -0.216833    -0.819997     0.605019  \n","\n","[28520 rows x 768 columns]      feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n","0    -0.014161  -0.069598   0.225868  -0.354306   0.915667   1.311790   \n","1     1.183196   0.680161   0.510583  -0.471411   1.526684   1.174273   \n","2     0.125244   0.205833   0.255090   0.482894  -0.454637   0.733924   \n","3     0.076462  -0.851154  -0.551902   0.144252   1.014501   0.949078   \n","4     2.926123   1.139308   2.678761  -1.180134  -0.876465  -2.413230   \n","..         ...        ...        ...        ...        ...        ...   \n","731   0.590582  -0.018932  -0.116632   0.324357   0.521993   0.608952   \n","732  -0.244734  -0.746615  -0.602970   0.561293  -0.022480  -0.182642   \n","733   0.082063   0.090612  -0.408616  -0.041506   0.810502   0.736369   \n","734  -0.130611  -0.146921  -0.604058   0.823415   0.606324   0.193136   \n","735   0.610706   0.123086   0.281524   0.428831  -0.098346   0.055940   \n","\n","     feature_7  feature_8  feature_9  feature_10  ...  feature_759  \\\n","0    -1.938703  -1.078428   0.078991    1.160951  ...    -0.541158   \n","1    -0.058888  -0.647836  -0.709157    2.294407  ...    -0.913198   \n","2    -0.467368  -0.310125  -0.215389    0.255770  ...    -0.107812   \n","3    -1.165740  -0.424997   0.611387    0.566706  ...     0.186220   \n","4     0.432502  -1.239525  -2.433489    1.152320  ...    -1.662766   \n","..         ...        ...        ...         ...  ...          ...   \n","731  -0.390113   0.343251   0.071084    0.014507  ...    -0.046697   \n","732  -0.566026   0.459833   0.234606   -0.481912  ...     0.486211   \n","733   0.289180   0.249246   0.287157   -0.290349  ...     0.277092   \n","734  -0.281387   0.325335   0.557237   -0.995742  ...     0.466774   \n","735  -0.071103   0.527636  -0.243998    0.257552  ...     0.167100   \n","\n","     feature_760  feature_761  feature_762  feature_763  feature_764  \\\n","0       0.776351    -0.632814    -0.973466     0.000957    -0.304360   \n","1       1.613942     1.675776    -0.712368     0.209305    -0.293204   \n","2      -0.067125     0.008978     0.547171    -0.346335     0.417590   \n","3      -0.286777    -1.133628     0.185260     0.319743    -0.491386   \n","4       1.011149    -0.254485    -2.607957    -2.485013    -1.062137   \n","..           ...          ...          ...          ...          ...   \n","731    -0.433035     0.586633     0.055289     0.466795     1.210931   \n","732    -0.516893    -0.538407     0.260536     0.278390     0.116072   \n","733    -0.369650     0.153890     0.111588    -0.054429     0.944325   \n","734    -0.964510    -0.586074     1.117095     0.936686     0.791300   \n","735     0.010977    -0.452550    -0.091229    -0.138328     0.562533   \n","\n","     feature_765  feature_766  feature_767  feature_768  \n","0       1.830431    -0.524634     0.355475     1.309780  \n","1      -0.383486    -0.429185     0.968116     1.232612  \n","2       0.350719    -0.383129    -0.306566    -0.155106  \n","3       0.893945    -0.448145     0.902828     0.608997  \n","4       0.507644     0.288881    -0.249331     1.722395  \n","..           ...          ...          ...          ...  \n","731     0.181143    -0.074448     0.371083     0.236448  \n","732    -0.372254    -0.578038     0.518517    -0.811947  \n","733     0.980826    -0.140487    -0.198699    -0.246964  \n","734     0.207986    -0.390037    -0.251415    -0.290289  \n","735     0.080541    -0.614546     0.158471    -0.010620  \n","\n","[736 rows x 768 columns] 480    25.0\n","481    25.0\n","482    25.0\n","483    25.0\n","484    25.0\n","       ... \n","475    27.0\n","476    26.0\n","477    30.0\n","478    31.0\n","479    31.0\n","Name: label_2, Length: 28520, dtype: float64 14     25.0\n","15     25.0\n","16     25.0\n","17     25.0\n","18     25.0\n","       ... \n","745    29.0\n","746    29.0\n","747    29.0\n","748    29.0\n","749    29.0\n","Name: label_2, Length: 736, dtype: float64\n"]}],"source":["# scaling\n","# scaling function\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import RobustScaler\n","def scale(train,valid_x,test,label):\n","    train_x_label_1=train.iloc[:, :-4]\n","    train_y_label_1=train.iloc[:,-5+label]\n","    \n","    valid_x_label_1=valid.iloc[:, :-4]\n","    valid_y_label_1=valid.iloc[:,-5+label]\n","    \n","    test_x_label_1=test.iloc[:, :-4]\n","    test_y_label_1=test.iloc[:,-5+label]\n","\n","    ss = RobustScaler()\n","    scaled_train_x_label = ss.fit_transform(train_x_label_1)\n","    scaled_test_x_label=ss.fit_transform(test_x_label_1)\n","    scaled_valid_x_label=ss.fit_transform(valid_x_label_1)\n","    \n","    scaled_train_x_label_df = pd.DataFrame(scaled_train_x_label,columns = train_x_label_1.columns)\n","    scaled_test_x_label_df=pd.DataFrame(scaled_test_x_label,columns=test_x_label_1.columns)\n","    scaled_valid_x_label_df=pd.DataFrame(scaled_valid_x_label,columns=valid_x_label_1.columns)\n","\n","    return scaled_train_x_label_df,scaled_valid_x_label_df,scaled_test_x_label_df,train_y_label_1,valid_y_label_1\n","\n","\n","\n","valid=pd.read_csv('/kaggle/input/ml-grp-project-layer-11/valid.csv')\n","valid=valid.dropna()\n","test=pd.read_csv('/kaggle/input/ml-grp-project-layer-11/test.csv')\n","scaled_train_x_label_2,scaled_valid_x_label_2,scaled_test_x_label_2,train_y_label_2,valid_y_label_2=scale(imputed_data,valid,test,2)\n","\n","# print(scaled_train_x_label_2,scaled_valid_x_label_2,train_y_label_2,valid_y_label_2)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T06:41:59.205774Z","iopub.status.busy":"2023-09-24T06:41:59.205471Z","iopub.status.idle":"2023-09-24T06:41:59.209821Z","shell.execute_reply":"2023-09-24T06:41:59.208776Z","shell.execute_reply.started":"2023-09-24T06:41:59.205746Z"},"trusted":true},"outputs":[],"source":["# remove  nan values in valid data set\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T06:41:59.212332Z","iopub.status.busy":"2023-09-24T06:41:59.211897Z","iopub.status.idle":"2023-09-24T06:41:59.224952Z","shell.execute_reply":"2023-09-24T06:41:59.224138Z","shell.execute_reply.started":"2023-09-24T06:41:59.212301Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","\n","def train_and_evaluate_models(X_train, y_train, X_val, y_val,test_x):\n"," \n","    models = {\n","        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n","        'SVM': SVC(kernel='linear', C=1.0, random_state=42),\n","        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n","#         'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n","#         'Naive Bayes': GaussianNB(),\n","#         'Decision Tree': DecisionTreeClassifier(random_state=42)\n","    }\n","\n","    accuracies = {}\n","    pred={}\n","    \n","    for model_name, model in models.items():\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_val)\n","        y_pred_test=model.predict(test_x)\n","        accuracy = accuracy_score(y_val, y_pred)\n","        accuracies[model_name] = accuracy\n","        pred[model_name]=y_pred_test\n","    print(accuracies)\n","    return accuracies,pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cross_validation(x_train,y_train,x_valid,y_valid):\n","    classifiers = [\n","    (\"Random Forest\", RandomForestClassifier()),\n","    ('Logistic Regression',LogisticRegression(max_iter=1000, random_state=42)),\n","    (\"K-Nearest Neighbors\", KNeighborsClassifier(n_neighbors=5)),\n","    (\"SVM\", SVC(kernel=\"linear\"))]\n","\n","\n","    \n","    for model_name, model in classifiers:\n","        cross_val_scores = cross_val_score(model, X_train, Y_train, cv=5)\n","        print(f\"{model_name} Cross-validation scores:\", cross_val_scores)\n","        print(f\"{model_name} Mean accuracy:\", cross_val_scores.mean())\n","        print(f\"{model_name} Standard deviation:\", cross_val_scores.std())\n","        print(\"\\n\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T06:41:59.226366Z","iopub.status.busy":"2023-09-24T06:41:59.226020Z","iopub.status.idle":"2023-09-24T06:41:59.271833Z","shell.execute_reply":"2023-09-24T06:41:59.270993Z","shell.execute_reply.started":"2023-09-24T06:41:59.226336Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>feature_6</th>\n","      <th>feature_7</th>\n","      <th>feature_8</th>\n","      <th>feature_9</th>\n","      <th>feature_10</th>\n","      <th>...</th>\n","      <th>feature_759</th>\n","      <th>feature_760</th>\n","      <th>feature_761</th>\n","      <th>feature_762</th>\n","      <th>feature_763</th>\n","      <th>feature_764</th>\n","      <th>feature_765</th>\n","      <th>feature_766</th>\n","      <th>feature_767</th>\n","      <th>feature_768</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.014161</td>\n","      <td>-0.069598</td>\n","      <td>0.225868</td>\n","      <td>-0.354306</td>\n","      <td>0.915667</td>\n","      <td>1.311790</td>\n","      <td>-1.938703</td>\n","      <td>-1.078428</td>\n","      <td>0.078991</td>\n","      <td>1.160951</td>\n","      <td>...</td>\n","      <td>-0.541158</td>\n","      <td>0.776351</td>\n","      <td>-0.632814</td>\n","      <td>-0.973466</td>\n","      <td>0.000957</td>\n","      <td>-0.304360</td>\n","      <td>1.830431</td>\n","      <td>-0.524634</td>\n","      <td>0.355475</td>\n","      <td>1.309780</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.183196</td>\n","      <td>0.680161</td>\n","      <td>0.510583</td>\n","      <td>-0.471411</td>\n","      <td>1.526684</td>\n","      <td>1.174273</td>\n","      <td>-0.058888</td>\n","      <td>-0.647836</td>\n","      <td>-0.709157</td>\n","      <td>2.294407</td>\n","      <td>...</td>\n","      <td>-0.913198</td>\n","      <td>1.613942</td>\n","      <td>1.675776</td>\n","      <td>-0.712368</td>\n","      <td>0.209305</td>\n","      <td>-0.293204</td>\n","      <td>-0.383486</td>\n","      <td>-0.429185</td>\n","      <td>0.968116</td>\n","      <td>1.232612</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.125244</td>\n","      <td>0.205833</td>\n","      <td>0.255090</td>\n","      <td>0.482894</td>\n","      <td>-0.454637</td>\n","      <td>0.733924</td>\n","      <td>-0.467368</td>\n","      <td>-0.310125</td>\n","      <td>-0.215389</td>\n","      <td>0.255770</td>\n","      <td>...</td>\n","      <td>-0.107812</td>\n","      <td>-0.067125</td>\n","      <td>0.008978</td>\n","      <td>0.547171</td>\n","      <td>-0.346335</td>\n","      <td>0.417590</td>\n","      <td>0.350719</td>\n","      <td>-0.383129</td>\n","      <td>-0.306566</td>\n","      <td>-0.155106</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.076462</td>\n","      <td>-0.851154</td>\n","      <td>-0.551902</td>\n","      <td>0.144252</td>\n","      <td>1.014501</td>\n","      <td>0.949078</td>\n","      <td>-1.165740</td>\n","      <td>-0.424997</td>\n","      <td>0.611387</td>\n","      <td>0.566706</td>\n","      <td>...</td>\n","      <td>0.186220</td>\n","      <td>-0.286777</td>\n","      <td>-1.133628</td>\n","      <td>0.185260</td>\n","      <td>0.319743</td>\n","      <td>-0.491386</td>\n","      <td>0.893945</td>\n","      <td>-0.448145</td>\n","      <td>0.902828</td>\n","      <td>0.608997</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.926123</td>\n","      <td>1.139308</td>\n","      <td>2.678761</td>\n","      <td>-1.180134</td>\n","      <td>-0.876465</td>\n","      <td>-2.413230</td>\n","      <td>0.432502</td>\n","      <td>-1.239525</td>\n","      <td>-2.433489</td>\n","      <td>1.152320</td>\n","      <td>...</td>\n","      <td>-1.662766</td>\n","      <td>1.011149</td>\n","      <td>-0.254485</td>\n","      <td>-2.607957</td>\n","      <td>-2.485013</td>\n","      <td>-1.062137</td>\n","      <td>0.507644</td>\n","      <td>0.288881</td>\n","      <td>-0.249331</td>\n","      <td>1.722395</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>731</th>\n","      <td>0.590582</td>\n","      <td>-0.018932</td>\n","      <td>-0.116632</td>\n","      <td>0.324357</td>\n","      <td>0.521993</td>\n","      <td>0.608952</td>\n","      <td>-0.390113</td>\n","      <td>0.343251</td>\n","      <td>0.071084</td>\n","      <td>0.014507</td>\n","      <td>...</td>\n","      <td>-0.046697</td>\n","      <td>-0.433035</td>\n","      <td>0.586633</td>\n","      <td>0.055289</td>\n","      <td>0.466795</td>\n","      <td>1.210931</td>\n","      <td>0.181143</td>\n","      <td>-0.074448</td>\n","      <td>0.371083</td>\n","      <td>0.236448</td>\n","    </tr>\n","    <tr>\n","      <th>732</th>\n","      <td>-0.244734</td>\n","      <td>-0.746615</td>\n","      <td>-0.602970</td>\n","      <td>0.561293</td>\n","      <td>-0.022480</td>\n","      <td>-0.182642</td>\n","      <td>-0.566026</td>\n","      <td>0.459833</td>\n","      <td>0.234606</td>\n","      <td>-0.481912</td>\n","      <td>...</td>\n","      <td>0.486211</td>\n","      <td>-0.516893</td>\n","      <td>-0.538407</td>\n","      <td>0.260536</td>\n","      <td>0.278390</td>\n","      <td>0.116072</td>\n","      <td>-0.372254</td>\n","      <td>-0.578038</td>\n","      <td>0.518517</td>\n","      <td>-0.811947</td>\n","    </tr>\n","    <tr>\n","      <th>733</th>\n","      <td>0.082063</td>\n","      <td>0.090612</td>\n","      <td>-0.408616</td>\n","      <td>-0.041506</td>\n","      <td>0.810502</td>\n","      <td>0.736369</td>\n","      <td>0.289180</td>\n","      <td>0.249246</td>\n","      <td>0.287157</td>\n","      <td>-0.290349</td>\n","      <td>...</td>\n","      <td>0.277092</td>\n","      <td>-0.369650</td>\n","      <td>0.153890</td>\n","      <td>0.111588</td>\n","      <td>-0.054429</td>\n","      <td>0.944325</td>\n","      <td>0.980826</td>\n","      <td>-0.140487</td>\n","      <td>-0.198699</td>\n","      <td>-0.246964</td>\n","    </tr>\n","    <tr>\n","      <th>734</th>\n","      <td>-0.130611</td>\n","      <td>-0.146921</td>\n","      <td>-0.604058</td>\n","      <td>0.823415</td>\n","      <td>0.606324</td>\n","      <td>0.193136</td>\n","      <td>-0.281387</td>\n","      <td>0.325335</td>\n","      <td>0.557237</td>\n","      <td>-0.995742</td>\n","      <td>...</td>\n","      <td>0.466774</td>\n","      <td>-0.964510</td>\n","      <td>-0.586074</td>\n","      <td>1.117095</td>\n","      <td>0.936686</td>\n","      <td>0.791300</td>\n","      <td>0.207986</td>\n","      <td>-0.390037</td>\n","      <td>-0.251415</td>\n","      <td>-0.290289</td>\n","    </tr>\n","    <tr>\n","      <th>735</th>\n","      <td>0.610706</td>\n","      <td>0.123086</td>\n","      <td>0.281524</td>\n","      <td>0.428831</td>\n","      <td>-0.098346</td>\n","      <td>0.055940</td>\n","      <td>-0.071103</td>\n","      <td>0.527636</td>\n","      <td>-0.243998</td>\n","      <td>0.257552</td>\n","      <td>...</td>\n","      <td>0.167100</td>\n","      <td>0.010977</td>\n","      <td>-0.452550</td>\n","      <td>-0.091229</td>\n","      <td>-0.138328</td>\n","      <td>0.562533</td>\n","      <td>0.080541</td>\n","      <td>-0.614546</td>\n","      <td>0.158471</td>\n","      <td>-0.010620</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>736 rows × 768 columns</p>\n","</div>"],"text/plain":["     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n","0    -0.014161  -0.069598   0.225868  -0.354306   0.915667   1.311790   \n","1     1.183196   0.680161   0.510583  -0.471411   1.526684   1.174273   \n","2     0.125244   0.205833   0.255090   0.482894  -0.454637   0.733924   \n","3     0.076462  -0.851154  -0.551902   0.144252   1.014501   0.949078   \n","4     2.926123   1.139308   2.678761  -1.180134  -0.876465  -2.413230   \n","..         ...        ...        ...        ...        ...        ...   \n","731   0.590582  -0.018932  -0.116632   0.324357   0.521993   0.608952   \n","732  -0.244734  -0.746615  -0.602970   0.561293  -0.022480  -0.182642   \n","733   0.082063   0.090612  -0.408616  -0.041506   0.810502   0.736369   \n","734  -0.130611  -0.146921  -0.604058   0.823415   0.606324   0.193136   \n","735   0.610706   0.123086   0.281524   0.428831  -0.098346   0.055940   \n","\n","     feature_7  feature_8  feature_9  feature_10  ...  feature_759  \\\n","0    -1.938703  -1.078428   0.078991    1.160951  ...    -0.541158   \n","1    -0.058888  -0.647836  -0.709157    2.294407  ...    -0.913198   \n","2    -0.467368  -0.310125  -0.215389    0.255770  ...    -0.107812   \n","3    -1.165740  -0.424997   0.611387    0.566706  ...     0.186220   \n","4     0.432502  -1.239525  -2.433489    1.152320  ...    -1.662766   \n","..         ...        ...        ...         ...  ...          ...   \n","731  -0.390113   0.343251   0.071084    0.014507  ...    -0.046697   \n","732  -0.566026   0.459833   0.234606   -0.481912  ...     0.486211   \n","733   0.289180   0.249246   0.287157   -0.290349  ...     0.277092   \n","734  -0.281387   0.325335   0.557237   -0.995742  ...     0.466774   \n","735  -0.071103   0.527636  -0.243998    0.257552  ...     0.167100   \n","\n","     feature_760  feature_761  feature_762  feature_763  feature_764  \\\n","0       0.776351    -0.632814    -0.973466     0.000957    -0.304360   \n","1       1.613942     1.675776    -0.712368     0.209305    -0.293204   \n","2      -0.067125     0.008978     0.547171    -0.346335     0.417590   \n","3      -0.286777    -1.133628     0.185260     0.319743    -0.491386   \n","4       1.011149    -0.254485    -2.607957    -2.485013    -1.062137   \n","..           ...          ...          ...          ...          ...   \n","731    -0.433035     0.586633     0.055289     0.466795     1.210931   \n","732    -0.516893    -0.538407     0.260536     0.278390     0.116072   \n","733    -0.369650     0.153890     0.111588    -0.054429     0.944325   \n","734    -0.964510    -0.586074     1.117095     0.936686     0.791300   \n","735     0.010977    -0.452550    -0.091229    -0.138328     0.562533   \n","\n","     feature_765  feature_766  feature_767  feature_768  \n","0       1.830431    -0.524634     0.355475     1.309780  \n","1      -0.383486    -0.429185     0.968116     1.232612  \n","2       0.350719    -0.383129    -0.306566    -0.155106  \n","3       0.893945    -0.448145     0.902828     0.608997  \n","4       0.507644     0.288881    -0.249331     1.722395  \n","..           ...          ...          ...          ...  \n","731     0.181143    -0.074448     0.371083     0.236448  \n","732    -0.372254    -0.578038     0.518517    -0.811947  \n","733     0.980826    -0.140487    -0.198699    -0.246964  \n","734     0.207986    -0.390037    -0.251415    -0.290289  \n","735     0.080541    -0.614546     0.158471    -0.010620  \n","\n","[736 rows x 768 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["scaled_valid_x_label_2\n","cross_validation(scaled_train_x_label_e,train_y_label_2,scaled_valid_x_label_2,valid_y_label_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#use knn without preprocessing\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import hamming_loss, jaccard_score\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# knn = KNeighborsClassifier(n_neighbors=5)  \n","\n","# Train the model\n","# knn.fit(scaled_train_x_label_2,train_y_label_2)\n","\n","# # Make predictions on the test set\n","# y_pred = knn.predict(scaled_valid_x_label_2)\n","# print(accuracy_score(valid_y_label_2, y_pred))\n","# print(classification_report(valid_y_label_2,y_pred))\n","\n","\n","train_and_evaluate_models(scaled_train_x_label_2,train_y_label_2,scaled_valid_x_label_2,valid_y_label_2,scaled_test_x_label_2)\n","# {'Random Forest': 0.7635869565217391,\n","#  'SVM': 0.7880434782608695,\n","#  'Logistic Regression': 0.7894021739130435,\n","#  'K-Nearest Neighbors': 0.8383152173913043,\n","#  'Naive Bayes': 0.3383152173913043,\n","#  'Decision Tree': 0.3654891304347826}"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import f_classif\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.model_selection import GridSearchCV\n","# Create a SelectKBest instance with a scoring function (e.g., chi-squared)\n","selector = SelectKBest(score_func=f_classif, k=400)  # Select the top 2 features\n","\n","# Fit and transform your data to select the best k features\n","scaled_train_x_label_2_df = selector.fit_transform(scaled_train_x_label_2_df, train_y_label_2)\n","scaled_valid_x_label_2_df = selector.transform(scaled_valid_x_label_2_df)\n","scaled_test_x_label_2_df = selector.transform(scaled_test_x_label_2_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acc,pred=train_and_evaluate_models(scaled_train_x_label_2_df,train_y_label_2,scaled_valid_x_label_2_df,valid_y_label_2,scaled_test_x_label_2_df)\n","print(\"accuracy after k best.\")\n","y_pred_test=pred['Logistic Regression']\n","write_predictions_to_csv(y_pred_test,\"lgistic regression label 1 layer 8 after k best.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import f_classif\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.model_selection import GridSearchCV\n","# Create a SelectKBest instance with a scoring function (e.g., chi-squared)\n","selector = SelectKBest(score_func=f_classif, k=400)  # Select the top 2 features\n","\n","# Fit and transform your data to select the best k features\n","scaled_train_x_label_2_df = selector.fit_transform(scaled_train_x_label_2_df,train_y_label_2)\n","scaled_valid_x_label_2_df = selector.transform(scaled_valid_x_label_2_df)\n","scaled_test_x_label_2_df = selector.transform(scaled_test_x_label_2_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["acc,pred=train_and_evaluate_models(scaled_train_x_label_2_df,train_y_label_2,scaled_valid_x_label_2_df,valid_y_label_2,scaled_test_x_label_2_df)\n","print(\"accuracy after k best.\")\n","y_pred_test=pred['Logistic Regression']\n","write_predictions_to_csv(y_pred_test,\"lgistic regression label 2 layer 11 after k best.csv\")\n","\n","y_pred_test=pred['SVM']\n","write_predictions_to_csv(y_pred_test,\"SVC label 2 layer 11 after k best.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-23T14:11:52.461712Z","iopub.status.idle":"2023-09-23T14:11:52.462089Z","shell.execute_reply":"2023-09-23T14:11:52.461918Z","shell.execute_reply.started":"2023-09-23T14:11:52.461901Z"},"trusted":true},"outputs":[],"source":["# correlated_features = set()\n","# correlation_matrix = scaled_train_x_label_2.corr()\n","# # print(correlation_matrix)\n","# for i in range(len(correlation_matrix .columns)):\n","#     for j in range(i):\n","#         if abs(correlation_matrix.iloc[i, j]) > 0.5:\n","#             colname = correlation_matrix.columns[i]\n","#             correlated_features.add(colname)\n","# # print(correlated_features)            \n","# scaled_train_x_label_2.drop(labels=correlated_features, axis=1, inplace=True)\n","# scaled_valid_x_label_2.drop(labels=correlated_features, axis=1, inplace=True)\n","# scaled_test_x_label_2.drop(labels=correlated_features, axis=1, inplace=True)\n","# print(scaled_train_x_label_2.shape,scaled_valid_x_label_2.shape)\n","# # knn(scaled_train_x_label_2,train_y_label_2,scaled_valid_x_label_2,valid_y_label_2)\n","# train_and_evaluate_models(scaled_train_x_label_2,train_y_label_2,scaled_valid_x_label_2,valid_y_label_2)\n","# after removing corelated features\n","# {'Random Forest': 0.7472826086956522,\n","#  'SVM': 0.7703804347826086,\n","#  'Logistic Regression': 0.7635869565217391,\n","#  'K-Nearest Neighbors': 0.8315217391304348,\n","#  'Naive Bayes': 0.3736413043478261,\n","#  'Decision Tree': 0.3845108695652174}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-23T14:11:52.463524Z","iopub.status.idle":"2023-09-23T14:11:52.463926Z","shell.execute_reply":"2023-09-23T14:11:52.463713Z","shell.execute_reply.started":"2023-09-23T14:11:52.463696Z"},"trusted":true},"outputs":[],"source":["# pca approch_\n","from sklearn.decomposition import PCA\n","def pca(train_x,train_y,valid_x,valid_y,test):\n","    pca=PCA(.95, svd_solver='full')\n","    pca=pca.fit(train_x)\n","    train_features_pca=pca.transform(train_x)\n","    valid_features_pca=pca.transform(valid_x)\n","    test_features_pca=pca.transform(test_x)\n","    return train_features_pca,valid_features_pca,test_features_pca\n","\n","\n","scaled_train_x_label_2_df_pca,scaled_valid_x_label_2_df_pca,scaled_test_x_label_2_df_pca=pca(scaled_train_x_label_2_df,train_y_label_2,scaled_valid_x_label_2_df,valid_y_label_2,scaled_test_x_label_2_df)\n","acc,pred=train_and_evaluate_models(scaled_train_x_label_2_df_pca,train_y_label_2,scaled_valid_x_label_2_df_pca,valid_y_label_2,scaled_test_x_label_2_df_pca)\n","print(\"accuracy after k best.\")\n","y_pred_test=pred['Logistic Regression']\n","y_pred_test_svc=pred['SVM']\n","write_predictions_to_csv(y_pred_test,\"logistic regression label 2 layer 11 after pca.csv\")\n","write_predictions_to_csv(y_pred_test_svc,\"SVM label 2 layer 11 after pca.csv\")\n","# after pca for the data set\n","# {'Random Forest': 0.7472826086956522,\n","#  'SVM': 0.7703804347826086,\n","#  'Logistic Regression': 0.7635869565217391,\n","#  'K-Nearest Neighbors': 0.8315217391304348,\n","#  'Naive Bayes': 0.3736413043478261,\n","#  'Decision Tree': 0.3845108695652174}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-23T14:11:52.465063Z","iopub.status.idle":"2023-09-23T14:11:52.465412Z","shell.execute_reply":"2023-09-23T14:11:52.465251Z","shell.execute_reply.started":"2023-09-23T14:11:52.465234Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = { \n","    'C': [0.1, 1, 10],\n","    'kernel': ['linear', 'rbf', 'poly'],\n","}\n","\n","grid_search = GridSearchCV(\n","    estimator=SVC(kernel=\"linear\"),\n","    param_grid=param_grid,\n","    scoring='accuracy',\n","    cv=5,\n","    n_jobs=-1\n",")\n","\n","# Fit the grid search to your data\n","grid_search.fit(scaled_train_x_label_2_df_pca, train_y_label_2)\n","# Get the best hyperparameters\n","best_params = grid_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)\n","\n","# Get the best model\n","best_model = grid_search.best_estimator_\n","\n","# Evaluate the best model on the validation data\n","accuracy = best_model.score(scaled_valid_x_label_2_df_pca, valid_y_label_1)\n","print(\"Validation Accuracy with Best Model:\", accuracy)\n","y_pred= best_model.predict(scaled_valid_x_label_2_df_pca)\n","y_pred_test=best_model_predict(scaled_test_x_label_2_df_pca)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_predictions_to_csv(y_pred_test, \"label_2_layer_11_svc_predictions_with_hyperameter_tuning.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":4}
