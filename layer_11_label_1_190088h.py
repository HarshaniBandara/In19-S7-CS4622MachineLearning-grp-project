# -*- coding: utf-8 -*-
"""layer-11-label-1-190088H.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-6vTPJVKnC4XYzsU2QtGWPPVX2eRnVXt
"""

import numpy as np
import pandas as pd
import sklearn as sk
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import GridSearchCV

def train_and_evaluate_models(X_train, y_train, X_val, y_val,test_x):
    """
    Train and evaluate multiple classification models on the given data.

    Parameters:
    - X_train: Training features
    - y_train: Training labels
    - X_val: Validation features
    - y_val: Validation labels

    Returns:
    - A dictionary containing model names as keys and their accuracies on the validation data as values.
    """

    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(kernel='linear', C=1.0, random_state=42),
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
        'Naive Bayes': GaussianNB(),
        'Decision Tree': DecisionTreeClassifier(random_state=42)
    }

    accuracies = {}
    pred={}

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        y_pred_test=model.predict(test_x)
        accuracy = accuracy_score(y_val, y_pred)
        accuracies[model_name] = accuracy
        pred[model_name]=y_pred_test
    print(accuracies)
    return accuracies,pred

def cross_validation(x_train,y_train,x_valid,y_valid):
    classifiers = [
    ("Random Forest", RandomForestClassifier()),
    ('Logistic Regression',LogisticRegression(max_iter=1000, random_state=42)),
    ("K-Nearest Neighbors", KNeighborsClassifier(n_neighbors=5)),
    ("SVM", SVC(kernel="linear"))]



    for model_name, model in classifiers:
        cross_val_scores = cross_val_score(model, x_train, y_train, cv=5)
        print(f"{model_name} Cross-validation scores:", cross_val_scores)
        print(f"{model_name} Mean accuracy:", cross_val_scores.mean())
        print(f"{model_name} Standard deviation:", cross_val_scores.std())
        print("\n")

#function for knn model using and check accuarcy

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
def knn(train_x,train_y,valid_x,valid_y):
    model = RandomForestClassifier(n_estimators=100)
    model.fit(train_x, train_y)
    y_pred = model.predict(valid_x)
    accuracy = accuracy_score(valid_y, y_pred)
    print(f'Accuracy using knn: {accuracy:.2f}')

#function for data preprocessing
from sklearn.preprocessing import StandardScaler
def preprocessing(label,test_x_label_1,valid_x_label_1):
    train=pd.read_csv("/kaggle/input/ml-grp-project-train-data-set/train.csv")
    test=pd.read_csv("/kaggle/input/ml-grp-project-test-data-set/test.csv")
    valid=pd.read_csv("/kaggle/input/ml-grp-project-test-data-set/valid.csv")
    train_x_label_1=train.iloc[:, :-4]
    train_y_label_1=train.iloc[:,-5+label]

    ss = StandardScaler()
    scaled_train_x_label = ss.fit_transform(train_x_label_1)
    scaled_train_x_label
    scaled_test_x_label=ss.fit_transform(test_x_label_1)
    scaled_valid_x_label=ss.fit_transform(valid_x_label_1)
    return scaled_train_x_label,scaled_valid_x_label

# pca approch
from sklearn.decomposition import PCA
def pca(train_x,train_y,valid_x,valid_y,test_x):
    pca=PCA(.95, svd_solver='full')
    pca=pca.fit(train_x)
    train_features_pca=pca.transform(train_x)
    valid_features_pca=pca.transform(valid_x)
    test_features_pca=pca.transform(test_x)
    print("accuarcy after pca")
    #     knn(train_features_pca,train_y,valid_features_pca,valid_y)
    return train_features_pca,valid_features_pca,test_features_pca

# Write predicted values to a CSV file.
import pandas as pd

def write_predictions_to_csv(predictions, output_file):


    # Create a DataFrame with a column for predictions
    df = pd.DataFrame({'Predicted_Label': predictions})

    # Save the DataFrame to a CSV file
    df.to_csv(output_file, index=False)  # Set index=False to exclude row numbers

train=pd.read_csv("/kaggle/input/ml-grp-project-layer-11/train.csv")
test=pd.read_csv("/kaggle/input/ml-grp-project-layer-11/test.csv")
valid=pd.read_csv("/kaggle/input/ml-grp-project-layer-11/valid.csv")

train.head()
test.head()
valid.head()
train.info()
train.dtypes

#Check is there any NaN values
train.isnull().sum()
#check is there any duplicates in the data set
train.drop_duplicates()

#only label_2 has NaN values

#cheack is there any string values, if there any string values we can encode the values.
contains_strings=train.applymap(lambda x: isinstance(x, str))
if contains_strings.any().any():
    print("There are string values in the DataFrame columns.")
else:
    print("There are no string values in the DataFrame columns.")

"""label 1"""

train_x_label_1=train.iloc[:, :-4]
train_y_label_1=train.iloc[:,-4:-3]
valid_x_label_1=valid.iloc[:, :-4]
valid_y_label_1=valid.iloc[:,-4:-3]
test_x_label_1=test.iloc[:, 1:]
train_x_label_1

# knn(train_x_label_1,train_y_label_1,valid_x_label_1,valid_y_label_1)
# Accuracy using knn: 0.88
test_x_label_1

import matplotlib.pyplot as plt
plt.hist(train_y_label_1['label_1'], bins=10, edgecolor='k')  # Adjust the number of bins as needed
plt.xlabel('label 1 values')
plt.ylabel('Frequency')
plt.title('Frequency Distribution of Target Values')
plt.grid(True)

# Show the plot
plt.show()

train_and_evaluate_models(train_x_label_1, train_y_label_1, valid_x_label_1,valid_y_label_1,test_x_label_1)
cross_validation(train_x_label_1, train_y_label_1, valid_x_label_1,valid_y_label_1)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
scaled_train_x_label_1 = ss.fit_transform(train_x_label_1)
scaled_train_x_label_1
scaled_test_x_label_1=ss.transform(test_x_label_1)
scaled_valid_x_label_1=ss.transform(valid_x_label_1)
train_and_evaluate_models(scaled_train_x_label_1, train_y_label_1, scaled_valid_x_label_1,valid_y_label_1,scaled_test_x_label_1)
print("""accuracy using standard scaling.{'SVM': 0.9386666666666666, 'Logistic Regression': 0.9626666666666667}""")

"""# accuracy using RobustScaler
from sklearn.preprocessing import RobustScaler
ss = RobustScaler()
scaled_train_x_label_1 = ss.fit_transform(train_x_label_1)
scaled_train_x_label_1
scaled_test_x_label_1=ss.transform(test_x_label_1)
scaled_valid_x_label_1=ss.transform(valid_x_label_1)
acc,pred=train_and_evaluate_models(scaled_train_x_label_1,train_y_label_1,scaled_valid_x_label_1,valid_y_label_1,scaled_test_x_label_1)
print("accuracy using RobustScaler.")
y_pred_test=pred['Logistic Regression']
"""

# accuracy using RobustScaler
from sklearn.preprocessing import RobustScaler
ss = RobustScaler()
scaled_train_x_label_1 = ss.fit_transform(train_x_label_1)
scaled_train_x_label_1
scaled_test_x_label_1=ss.transform(test_x_label_1)
scaled_valid_x_label_1=ss.transform(valid_x_label_1)
acc,pred=train_and_evaluate_models(scaled_train_x_label_1,train_y_label_1,scaled_valid_x_label_1,valid_y_label_1,scaled_test_x_label_1)
print("accuracy using RobustScaler.")
y_pred_test=pred['SVM']

# {'SVM': 0.932, 'Logistic Regression': 0.9493333333333334}
# accuracy using RobustScaler.

write_predictions_to_csv(y_pred_test,"svm label 1 layer 11 after scaling.csv")

scaled_train_x_label_1_df = pd.DataFrame(scaled_train_x_label_1,columns = train_x_label_1.columns)
scaled_train_x_label_1_df.describe()
scaled_test_x_label_1_df=pd.DataFrame(scaled_test_x_label_1,columns=test_x_label_1.columns)
scaled_valid_x_label_1_df=pd.DataFrame(scaled_valid_x_label_1,columns=valid_x_label_1.columns)

scaled_train_x_label_1_df.describe()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split



# model = RandomForestClassifier(n_estimators=100)
# model.fit(scaled_train_x_label_1, train_y_label_1)
# y_pred = model.predict(scaled_valid_x_label_1_df)
# from sklearn.metrics import accuracy_score

# # Calculate the accuracy
# accuracy = accuracy_score(valid_y_label_1, y_pred)
# print(f'Accuracy: {accuracy:.2f}')
# Accuracy: 0.88

from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import GridSearchCV
# Create a SelectKBest instance with a scoring function (e.g., chi-squared)
selector = SelectKBest(score_func=f_classif, k=400)  # Select the top 2 features

# Fit and transform your data to select the best k features
scaled_train_x_label_1_df = selector.fit_transform(scaled_train_x_label_1_df, train_y_label_1)
scaled_valid_x_label_1_df = selector.transform(scaled_valid_x_label_1_df)
scaled_test_x_label_1_df = selector.transform(scaled_test_x_label_1_df)

acc,pred=train_and_evaluate_models(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1,scaled_test_x_label_1_df)
print("accuracy after k best.")
y_pred_test=pred['SVM']
y_pred_test_log=pred['Logistic Regression']
write_predictions_to_csv(y_pred_test,"svm label 1 layer 11 after k best.csv")
write_predictions_to_csv(y_pred_test_log,"logistic reg label 1 layer 11 after k best .csv")

scaled_train_x_label_1_df_pca,scaled_valid_x_label_1_df_pca,scaled_test_x_label_1_df_pca=pca(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1,scaled_test_x_label_1_df)
acc,pred=train_and_evaluate_models(scaled_train_x_label_1_df_pca,train_y_label_1,scaled_valid_x_label_1_df_pca,valid_y_label_1,scaled_test_x_label_1_df_pca)
print("accuracy after k best.")
y_pred_test=pred['SVM']
write_predictions_to_csv(y_pred_test,"svm label 1 layer 11 after pca.csv")

y_pred_test_log=pred['Logistic Regression']
write_predictions_to_csv(y_pred_test_log,"logistic reg label 1 layer 11 after pca .csv")

correlated_features = set()
correlation_matrix = scaled_train_x_label_1_df.corr()
# print(correlation_matrix)
for i in range(len(correlation_matrix .columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.5:
            colname = correlation_matrix.columns[i]
            correlated_features.add(colname)
# print(correlated_features)
scaled_train_x_label_1_df.drop(labels=correlated_features, axis=1, inplace=True)
scaled_valid_x_label_1_df.drop(labels=correlated_features, axis=1, inplace=True)
print(scaled_train_x_label_1_df.describe(),scaled_valid_x_label_1_df.describe())
knn(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1)

#accuracy removing features 0.86

train_and_evaluate_models(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1)
print("Before pca accuaracy")
"""
{'Random Forest': 0.8546666666666667,
'SVM': 0.928,
'Logistic Regression': 0.9453333333333334,
'K-Nearest Neighbors': 0.8346666666666667,
'Naive Bayes': 0.5933333333333334,
'Decision Tree': 0.344}
"""

# use pca
pca(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1)

scaled_train_x_label_1_df_pca,scaled_valid_x_label_1_df_pca=pca(scaled_train_x_label_1_df,train_y_label_1,scaled_valid_x_label_1_df,valid_y_label_1)

print("after PCA")
train_and_evaluate_models(scaled_train_x_label_1_df_pca,train_y_label_1,scaled_valid_x_label_1_df_pca,valid_y_label_1)

"""{'Random Forest': 0.7466666666666667,
 'SVM': 0.9053333333333333,
 'Logistic Regression': 0.9346666666666666,
 'K-Nearest Neighbors': 0.8253333333333334,
 'Naive Bayes': 0.6893333333333334,
 'Decision Tree': 0.24666666666666667}
 after pca results

"""

from sklearn.linear_model import LogisticRegression
model=LogisticRegression(max_iter=1000, random_state=42)
model.fit(scaled_train_x_label_1_df_pca,train_y_label_1)
y_pred = model.predict(scaled_valid_x_label_1_df_pca)
y_pred

# hyper parameter tuninng
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define hyperparameter grid
param_grid = {
    'C': [1, 10, 100],  # Adjust the range based on your needs
    'penalty': ['l1', 'l2'],
    'solver': ['lbfgs'],
#     'max_iter': [100, 200, 300]  # Adjust the range based on your needs
}

# Create a Logistic Regression classifier
logistic_regression = LogisticRegression()

# Perform Grid Search with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, scoring='accuracy', cv=5)

# Fit the Grid Search to your training data
grid_search.fit(scaled_train_x_label_1_df_pca,train_y_label_1)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the validation data
accuracy = best_model.score(scaled_valid_x_label_1_df_pca, valid_y_label_1)
print("Validation Accuracy with Best Model:", accuracy)
y_pred= best_model.predict(scaled_valid_x_label_1_df_pca)
y_pred_test=best_model.predict(scaled_test_x_label_1_df_pca)

# write to the csv file
write_predictions_to_csv(y_pred_test, "label_1_layer_11_logistic_regression_predictions_with_hyperameter_tuning.csv")

y_pred_test